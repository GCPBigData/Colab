{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Heuristic_user_segmentation.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhd4HlVVovTf","colab_type":"text"},"source":["##Heuristic user segmentation"]},{"cell_type":"code","metadata":{"id":"eBEUNL8xohqj","colab_type":"code","colab":{}},"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n","!pip install -q findspark"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnzHRhbNoi2_","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n","import findspark\n","findspark.init()\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SQLContext, SparkSession\n","from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n","from time import sleep\n","from pyspark.streaming import StreamingContext\n","#Spark Contexto\n","sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n","from pyspark.sql import SparkSession\n","#Spark Session\n","ss = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark create REDIS HEMOCE\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()\n","    \n","sc.version "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"006YOye7olWw","colab_type":"code","colab":{}},"source":["import os\n","import time\n","from pyspark import SparkContext, SparkConf\n","from pyspark.streaming import StreamingContext\n","\n","# Você também precisa usar essas bibliotecas específicas\n","from ua_parser import user_agent_parser\n","from hyperloglog import HyperLogLog"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERYEvreFooUJ","colab_type":"code","colab":{}},"source":["sc = SparkContext(master='local[4]')\n","\n","# Preparando lotes com os dados de entrada\n","DATA_PATH = \"/data/course4/uid_ua_100k_splitted_by_5k\"\n","batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n","\n","# Criando Dstream para emular a geração de dados em tempo real\n","BATCH_TIMEOUT = 5 # Tempo limite entre a geração do lote\n","ssc = StreamingContext(sc, BATCH_TIMEOUT)\n","dstream = ssc.queueStream(rdds=batches)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLx2e-Azww70","colab_type":"code","colab":{}},"source":["finished = False\n","printed = False\n","\n","def set_ending_flag(rdd):\n","    global finished\n","    if rdd.isEmpty():\n","        finished = True\n","\n","def print_only_at_the_end(rdd):\n","    global printed\n","    if finished and not printed:\n","        # Digite seu código para classificar e imprimir o RDD resultante\n","        \n","        printed = True\n","\n","# Se tivermos recebido um vazio, o fluxo será concluído.\n","# Então imprima o resultado e pare o contexto.\n","\n","dstream.foreachRDD(set_ending_flag)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqyv-9MLw5O3","colab_type":"code","colab":{}},"source":["ssc.checkpoint('./checkpoint{}'.format(time.strftime(\"%Y_%m_%d_%H_%M_%s\", time.gmtime())))         \n","ssc.start()\n","while not printed:\n","    time.sleep(0.1)\n","ssc.stop()  # quando o resultado for impresso, pare o contexto do Spark Streaming\n","sc.stop()   # para o contexto do Spark para poder reiniciar o código sem reiniciar o kernel"],"execution_count":0,"outputs":[]}]}