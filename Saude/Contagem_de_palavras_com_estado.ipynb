{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Contagem_de_palavras_com_estado.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IMA2jcTDy2WF","colab_type":"code","colab":{}},"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n","!pip install -q findspark"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oosvsVQmzGya","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n","import findspark\n","findspark.init()\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SQLContext, SparkSession\n","from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n","from time import sleep\n","from pyspark.streaming import StreamingContext\n","#Spark Contexto\n","sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n","from pyspark.sql import SparkSession\n","#Spark Session\n","ss = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark create REDIS HEMOCE\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()\n","    \n","sc.version "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3u5oPow7zU8v","colab_type":"code","colab":{}},"source":["import os\n","import time\n","from pyspark import SparkContext, SparkConf\n","from pyspark.streaming import StreamingContext"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRax-aduzX32","colab_type":"code","colab":{}},"source":["# Preparing SparkContext\n","sc = SparkContext(master='local[4]')\n","\n","# Preparing batches with the input data\n","DATA_PATH = \"/data/wiki/en_articles_streaming\"\n","\n","batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n","\n","# Creating Dstream to emulate realtime data generating\n","BATCH_TIMEOUT = 5  # Timeout between batch generation\n","ssc = StreamingContext(sc, BATCH_TIMEOUT)\n","dstream = ssc.queueStream(rdds=batches)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FT87qrLLzeaf","colab_type":"code","colab":{}},"source":["finished = False\n","printed = False\n","\n","\n","def set_ending_flag(rdd):\n","    global finished\n","    if rdd.isEmpty():\n","        finished = True\n","\n","\n","def print_only_at_the_end(rdd):\n","    global printed\n","    # Type your code for printing the sorted data from the stream in a loop\n","    \n","    if finished and not printed:\n","        # Type your code for printing the sorted data from the stream in a loop\n","        \n","        printed = True\n","\n","\n","# If we have received an empty rdd, the stream is finished.\n","# So print the result and stop the context.\n","dstream.foreachRDD(set_ending_flag)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6IGVwNizqpP","colab_type":"code","colab":{}},"source":["ssc.checkpoint('./checkpoint{}'.format(time.strftime(\"%Y_%m_%d_%H_%M_%s\", time.gmtime())))  # checkpoint for storing current state        \n","ssc.start()\n","while not printed:\n","    pass\n","ssc.stop()  # when the result was printed, stop SparkStreaming context\n","sc.stop()  # stop Spark context to be able restart the code without restarting the kernel"],"execution_count":0,"outputs":[]}]}