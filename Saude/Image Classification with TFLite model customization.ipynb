{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Image Classification with TFLite model customization.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_customization/demo/image_classification.ipynb","timestamp":1573030625719},{"file_id":"1Yg8COhKoiCSyf8sL4nTUSFI5iTnG6usg","timestamp":1570624529844}],"private_outputs":true,"collapsed_sections":[],"last_runtime":{"build_target":"","kind":"local"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h2q27gKz1H20"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"TUfAcER1oUS6","colab":{}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gb7qyhNL1yWt"},"source":["# Image classification with TensorFlow Lite model customization with TensorFlow 2.0"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nDABAblytltI"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_customization/demo/image_classification.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n","    Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_customization/demo/image_classification.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n","    View source on GitHub</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m86-Nh4pMHqY"},"source":["A biblioteca de personalização do modelo simplifica o processo de adaptação e conversão de um modelo de rede neural do TensorFlow em dados de entrada específicos ao implantar esse modelo para aplicativos ML no dispositivo.\n","\n","Este notebook mostra um exemplo de ponta a ponta que utiliza essa biblioteca de personalização de modelo para ilustrar a adaptação e conversão de um modelo de classificação de imagem comumente usado para classificar flores em um dispositivo móvel."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bcLF2PKkSbV3"},"source":["## Pré-requisitos\n","\n","Para executar este exemplo, primeiro precisamos instalar os pacotes necessários para o servidor, incluindo o pacote de personalização de modelos que no github [repo] (https://github.com/tensorflow/examples)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6cv3K3oaksJv","colab":{}},"source":["%tensorflow_version 2.x\n","!pip install -q git+https://github.com/tensorflow/examples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gx1HGRoFQ54j"},"source":["Import the required packages."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XtxiUeZEiXpt","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","\n","from tensorflow_examples.lite.model_customization.core.data_util.image_dataloader import ImageClassifierDataLoader\n","from tensorflow_examples.lite.model_customization.core.task import image_classifier\n","from tensorflow_examples.lite.model_customization.core.task.model_spec import efficientnet_b0_spec\n","from tensorflow_examples.lite.model_customization.core.task.model_spec import ImageModelSpec\n","\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KKRaYHABpob5"},"source":["## Exemplo simples de ponta a ponta\n","\n","Vamos usar algumas imagens para jogar com este exemplo simples de ponta a ponta. Você pode substituí-lo por suas próprias pastas de imagens. Centenas de imagens são um bom começo para a personalização do modelo, enquanto mais dados podem alcançar uma melhor precisão."]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"3jz5x0JoskPv","colab":{}},"source":["image_path = tf.keras.utils.get_file(\n","      'flower_photos',\n","      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n","      untar=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w-VDriAdsowu"},"source":["O exemplo consiste apenas em 4 linhas de código, como mostrado abaixo, cada uma representando uma etapa do processo geral."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6ahtcO86tZBL"},"source":["1. Carregue dados de entrada específicos para um aplicativo ML no dispositivo."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lANoNS_gtdH1","colab":{}},"source":["data = ImageClassifierDataLoader.from_folder(image_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y_9IWyIztuRF"},"source":["2. Personalize o modelo TensorFlow."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yRXMZbrwtyRD","colab":{}},"source":["model = image_classifier.create(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oxU2fDr-t2Ya"},"source":["3. Avalie o modelo."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wQr02VxJt6Cs","colab":{}},"source":["loss, accuracy = model.evaluate()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eVZw9zU8t84y"},"source":["4. Exporte para o modelo TensorFlow Lite."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zb-eIzfluCoa","colab":{}},"source":["model.export('image_classifier.tflite', 'image_labels.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pyju1qc_v-wy"},"source":["Após essas 4 etapas simples, poderíamos usar ainda o arquivo de modelo e o rótulo do modelo TensorFlow Lite em aplicativos no dispositivo, como em [classificação da imagem] (https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification ) aplicativo de referência."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R1QG32ivs9lF"},"source":["## Processo detalhado\n","\n","Atualmente, incluímos apenas os modelos MobileNetV2 e EfficientNetB0 como modelos pré-treinados para classificação de imagens. Mas é muito flexível adicionar novos modelos pré-treinados a esta biblioteca com apenas algumas linhas de código.\n","\n","\n","A seguir, passo a passo neste exemplo de ponta a ponta para mostrar mais detalhes."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ygEncJxtl-nQ"},"source":["### Etapa 1: carregar dados de entrada específicos para um aplicativo ML no dispositivo\n","\n","O conjunto de dados da flor contém 3670 imagens pertencentes a 5 classes. Baixe a versão de arquivo morto do conjunto de dados e descompacte-a.\n","\n","O conjunto de dados tem a seguinte estrutura de diretórios:\n","\n","<pre>\n","<b>flower_photos</b>\n","|__ <b>daisy</b>\n","    |______ 100080576_f52e8ee070_n.jpg\n","    |______ 14167534527_781ceb1b7a_n.jpg\n","    |______ ...\n","|__ <b>dandelion</b>\n","    |______ 10043234166_e6dd915111_n.jpg\n","    |______ 1426682852_e62169221f_m.jpg\n","    |______ ...\n","|__ <b>roses</b>\n","    |______ 102501987_3cdb8e5394_n.jpg\n","    |______ 14982802401_a3dfb22afb.jpg\n","    |______ ...\n","|__ <b>sunflowers</b>\n","    |______ 12471791574_bb1be83df4.jpg\n","    |______ 15122112402_cafa41934f.jpg\n","    |______ ...\n","|__ <b>tulips</b>\n","    |______ 13976522214_ccec508fe7.jpg\n","    |______ 14487943607_651e8062a1_m.jpg\n","    |______ ...\n","</pre>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7tOfUr2KlgpU","colab":{}},"source":["image_path = tf.keras.utils.get_file(\n","      'flower_photos',\n","      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n","      untar=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E051HBUM5owi"},"source":["Use a classe `ImageClassifierDataLoader` para carregar dados.\n","\n","Quanto ao método `from_folder ()`, ele pode carregar dados da pasta. Ele pressupõe que os dados de imagem da mesma classe estejam no mesmo subdiretório e o nome da subpasta seja o nome da classe. Atualmente, imagens codificadas em JPEG e imagens codificadas em PNG são suportadas."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"I_fOlZsklmlL","colab":{}},"source":["data = ImageClassifierDataLoader.from_folder(image_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z9_MYPie3EMO"},"source":["Mostre 30 exemplos de imagens com etiquetas."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ih4Wx44I482b","colab":{}},"source":["plt.figure(figsize=(20,20))\n","for i, (image, label) in enumerate(data.dataset.take(30)):\n","  plt.subplot(10,10,i+1)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.grid(False)\n","  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n","  plt.xlabel(data.index_to_label[label.numpy()])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AWuoensX4vDA"},"source":["Etapa 2: personalizar o modelo TensorFlow\n","\n","Crie um modelo de classificador de imagem personalizado com base nos dados carregados. O modelo padrão é MobileNetV2."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TvYSUuJY3QxR","colab":{}},"source":["model = image_classifier.create(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4JFOKWnH9x8_"},"source":["Veja a estrutura detalhada do modelo."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QNXAfjl192dC","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LP5FPk_tOxoZ"},"source":["### Etapa 3: avaliar o modelo personalizado\n","\n","Avalie o resultado do modelo, obtenha a perda e a precisão do modelo.\n","\n","Por padrão, os resultados são avaliados nos dados de teste divididos no método `create`. Outros dados de teste também podem ser avaliados se servidos como parâmetro."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A8c2ZQ0J3Riy","colab":{}},"source":["loss, accuracy = model.evaluate()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6ZCrYOWoCt05"},"source":["Podemos plotar os resultados previstos em 100 imagens de teste. Rótulos previstos com cor vermelha são os resultados previstos incorretos, enquanto outros estão corretos."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n9O9Kx7nDQWD","colab":{}},"source":["# Uma função auxiliar que retorna 'vermelho' / 'preto', dependendo de suas duas entradas\n","# parâmetro corresponde ou não.\n","def get_label_color(val1, val2):\n","  if val1 == val2:\n","    return 'green'\n","  else:\n","    return 'red'\n","\n","# Em seguida, plote 100 imagens de teste e seus rótulos previstos.\n","# Se um resultado de previsão for diferente do rótulo fornecido em \"teste\"\n","# dataset, destacaremos em vermelho.\n","plt.figure(figsize=(20, 20))\n","for i, (image, label) in enumerate(model.test_data.dataset.take(100)):\n","  ax = plt.subplot(10, 10, i+1)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.grid(False)\n","  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n","\n","# O pré-processamento deve permanecer o mesmo. Atualmente, apenas normalize cada valor de pixel para [0, 1] \n","# e redimensione a imagem para [224, 224, 3].\n","  image, label = model.preprocess_image(image, label)\n","# Adicione a dimensão do lote e converta para float32 para corresponder à entrada do modelo\n","# formato de dados.\n","  image = tf.expand_dims(image, 0).numpy()\n","\n","  predict_prob = model.model.predict(image)\n","  predict_label = np.argmax(predict_prob, axis=1)[0]\n","\n","  ax.xaxis.label.set_color(get_label_color(predict_label,\\\n","                                           label.numpy()))\n","  plt.xlabel('Predicted: %s' % model.test_data.index_to_label[predict_label])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S3H0rkbLUZAG"},"source":["Se a precisão não atender aos requisitos do aplicativo, é possível consultar [Uso avançado] (# scrollTo = zNDBP2qA54aK) para explorar alternativas, como mudar para um modelo maior, ajustar os parâmetros de re-treinamento etc."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aeHoGAceO2xV"},"source":["Etapa 4: Exportar para o modelo TensorFlow Lite\n","\n","Converta o modelo existente no formato de modelo TensorFlow Lite e salve os rótulos da imagem no arquivo de etiqueta."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Im6wA9lK3TQB","colab":{}},"source":["model.export('flower_classifier.tflite', 'flower_labels.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ROS2Ay2jMPCl"},"source":["O arquivo de modelo e o rótulo do modelo TensorFlow Lite podem ser usados no aplicativo de referência [classificação da imagem] (https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification).\n","\n","Quanto ao aplicativo de referência para Android como exemplo, podemos adicionar `flower_classifier.tflite` e` flower_label.txt` em [assets] (https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ pasta android / app / src / main / assets). Enquanto isso, altere o nome do arquivo do rótulo em [code] (https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/ rating / tflite / ClassifierFloatMobileNet.java # L65) e nome do arquivo TensorFlow Lite em [code] (https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/ java / org / tensorflow / lite / exemplos / classificação / tflite / ClassifierFloatMobileNet.java # L60). Assim, poderíamos executar o modelo TensorFlow Lite flutuante reciclado no aplicativo Android."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-4jQaxyT5_KV"},"source":["Aqui, também demonstramos como usar os arquivos acima para executar e avaliar o modelo TensorFlow Lite."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S1YoPX5wOK-u","colab":{}},"source":["# Read TensorFlow Lite model from TensorFlow Lite file.\n","with tf.io.gfile.GFile('flower_classifier.tflite', 'rb') as f:\n","  model_content = f.read()\n","\n","# Read label names from label file.\n","with tf.io.gfile.GFile('flower_labels.txt', 'r') as f:\n","  label_names = f.read().split('\\n')\n","\n","# Initialze TensorFlow Lite inpterpreter.\n","interpreter = tf.lite.Interpreter(model_content=model_content)\n","interpreter.allocate_tensors()\n","input_index = interpreter.get_input_details()[0]['index']\n","output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n","\n","# Run predictions on each test image data and calculate accuracy.\n","accurate_count = 0\n","for i, (image, label) in enumerate(model.test_data.dataset):\n","    # Pre-processing should remain the same. Currently, just normalize each pixel value and resize image according to the model's specification.\n","    image, label = model.preprocess_image(image, label)\n","    # Add batch dimension and convert to float32 to match with the model's input\n","    # data format.\n","    image = tf.expand_dims(image, 0).numpy()\n","\n","    # Run inference.\n","    interpreter.set_tensor(input_index, image)\n","    interpreter.invoke()\n","\n","    # Post-processing: remove batch dimension and find the label with highest\n","    # probability.\n","    predict_label = np.argmax(output()[0])\n","    # Get label name with label index.\n","    predict_label_name = label_names[predict_label]\n","\n","    accurate_count += (predict_label == label.numpy())\n","\n","accuracy = accurate_count * 1.0 / model.test_data.size\n","print('TensorFlow Lite model accuracy = %.4f' % accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fuHB-NFqpKTD"},"source":["Observe que o pré-processamento para inferência deve ser o mesmo que treinamento. Atualmente, o pré-processamento contém a normalização de cada valor de pixel e o redimensionamento da imagem de acordo com as especificações do modelo. Para o MobileNetV2, a imagem de entrada deve ser normalizada para `[0, 1]` e redimensionada para `[224, 224, 3]`."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zNDBP2qA54aK"},"source":["## Uso avançado\n","\n","A função `create` é a parte crítica desta biblioteca. Ele usa o aprendizado de transferência com um modelo pré-treinado semelhante ao [tutorial] (https://www.tensorflow.org/tutorials/images/transfer_learning).\n","\n","A função `create` contém os seguintes passos:\n","\n","1. Divida os dados em treinamento, validação e dados de teste de acordo com os parâmetros `validation_ratio` e` test_ratio`. O valor padrão de `validation_ratio` e` test_ratio` é `0.1` e` 0.1`.\n","2. Faça o download de um [Vetor de recurso de imagem] (https://www.tensorflow.org/hub/common_signatures/images#image_feature_vector) como modelo base no TensorFlow Hub. O modelo pré-treinado padrão é o MobileNetV2.\n","3. Adicione um cabeçote classificador com uma camada de dropout com `dropout_rate` entre a camada de head e o modelo pré-treinado. O padrão `dropout_rate` é` 0.2`.\n","4. Pré-processe os dados de entrada brutos. Atualmente, as etapas de pré-processamento incluem a normalização do valor de cada pixel da imagem para modelar a escala de entrada e redimensioná-lo para modelar o tamanho da entrada. O MobileNetV2 possui a escala de entrada `[0, 1]` e o tamanho da imagem de entrada `[224, 224, 3]`.\n","5. Alimente os dados no modelo do classificador. Por padrão, o número de épocas de treinamento é `2`, o tamanho do lote é` 32` e apenas o chefe do classificador é treinado.\n","\n","\n","Nesta seção, descrevemos vários tópicos avançados, incluindo a mudança para um modelo de classificação de imagem diferente, a alteração dos hiperparâmetros de treinamento etc."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"A4kiTJtZ_sDm"},"source":["## Change the model\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"794vgj6ud7Ep"},"source":["### Altere para o modelo suportado nesta biblioteca.\n","\n","Esta biblioteca suporta os modelos MobileNetV2 e EfficientNetB0 até agora. O modelo padrão é MobileNetV2.\n","\n","[EfficientNets] (https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet) são uma família de modelos de classificação de imagens que podem obter precisão de última geração. O EfficinetNetB0 é um dos modelos EfficientNet que é pequeno e adequado para aplicações no dispositivo. É maior que o MobileNetV2, enquanto pode alcançar um melhor desempenho.\n","\n","Poderíamos mudar o modelo para EfficientNetB0 apenas configurando o parâmetro `model_spec` como `fficientnet_b0_spec` no método` create`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7JKsJ6-P6ae1","colab":{}},"source":["model = image_classifier.create(data, model_spec=efficientnet_b0_spec)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gm_B1Wv08AxR"},"source":["Evaluate the newly retrained EfficientNetB0 model to see the accuracy and loss in testing data."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gQjqmyrJdrZE"},"source":[""]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lB2Go3HW8X7_","colab":{}},"source":["loss, accuracy = model.evaluate()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vAciGzVWtmWp"},"source":["### Alterar para o modelo no TensorFlow Hub\n","\n","Além disso, também poderíamos mudar para outros novos modelos que inserem uma imagem e produzem um vetor de recurso no formato TensorFlow Hub.\n","\n","Como o modelo [Inception V3] (https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1) como exemplo, poderíamos definir `inception_v3_spec`, que é um objeto do` ImageModelSpec` e contém a especificação do Inception Modelo V3.\n","\n","Precisamos especificar o nome do modelo `name`, o URL do modelo do TensorFlow Hub` uri`, a versão TensorFlow do modelo `tf_version`. Enquanto isso, o valor padrão de `input_image_shape` é` [224, 224] `. Precisamos alterá-lo para `[299, 299]` para o modelo Inception V3."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xdiMF2WMfAR4","colab":{}},"source":["inception_v3_spec = ImageModelSpec(\n","    name='inception_v3',\n","    uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1',\n","    tf_version=1)\n","inception_v3_spec.input_image_shape = [299, 299]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T_GGIoXZCs5F"},"source":["Then, by setting parameter model_spec to `inception_v3_spec` in `create` method, we could retrain the Inception V3 model. \n","\n","The remaining steps are exactly same and we could get a customized InceptionV3 TensorFlow Lite model in the end."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UhZ5IRKdeex3"},"source":["### Change your own custom model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"svTjlZhrCrcV"},"source":["If we'd like to use the custom model that's not in TensorFlow Hub, we should create and export [ModelSpec](https://www.tensorflow.org/hub/api_docs/python/hub/ModuleSpec) in TensorFlow Hub.\n","\n","Then start to define `ImageModelSpec` object like the process above."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A3k7mhH54QcK","colab":{}},"source":["model = image_classifier.create(data, epochs=5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VaYBQymQDsXU"},"source":["Evaluate the newly retrained model with 5 training epochs."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VafIYpKWD4Sw","colab":{}},"source":["loss, accuracy = model.evaluate()"],"execution_count":0,"outputs":[]}]}